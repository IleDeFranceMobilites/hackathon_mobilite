{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1817534a-aa76-48e6-a4bc-5c625676f5e1",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "Nous vous proposons dans ce notebook un template alimentant notre base de données vectorielle (ici Elastic Search) fonctionnant sur la plateforme Onyxia et l'environement Azure disponible chez IDFM.\n",
    "\n",
    "## Langchain\n",
    "\n",
    "Ce template est basé sur [langchain](https://python.langchain.com/docs/introduction/), librairie devenue un standard dans l'utilisation des LLM en général.\n",
    "Le tutoriel de langchain disponible en suivant ce [lien](https://python.langchain.com/docs/tutorials/rag/)\n",
    "\n",
    "## Fonctionnement\n",
    "\n",
    "Un RAG est une technique permettant d'enrichir les connaissances des LLM avec des données supplémentaires.\n",
    "\n",
    "Les LLM peuvent raisonner sur des sujets très variés, mais leurs connaissances sont limitées aux données qui ont été utilisées pour leur entraînement. Si vous souhaitez créer des applications d'IA capables de traiter des données privées ou des données introduites après la date limite d'un modèle, vous devez enrichir les connaissances de celui-ci avec les informations spécifiques dont il a besoin. Le processus consistant à apporter les informations appropriées et à les insérer dans l'invite du modèle est connu sous le nom de \"Retrieval Augmented Generation\" (RAG).\n",
    "\n",
    "LangChain dispose d'un certain nombre de composants conçus pour faciliter la création d'applications de questions-réponses et, plus généralement, d'applications de RAG.\n",
    "\n",
    "Le fonctionnement standard d'un RAG fait intervenir deux étapes :\n",
    "  - **L'indexation :** Phase au cours de laquelle nous déposons et indexons notre corpus de documents dans la base de données utilisée.\n",
    "  - **Le retrieval & generation :** Phase qui nous permet d'executer le RAG et de l'appeler avec un prompt.\n",
    "\n",
    "Ici, nous allons voir la partie **Indexation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a903f5-0509-495a-90c2-2f904d1b2599",
   "metadata": {},
   "source": [
    "## Indexation\n",
    "\n",
    "L'indexation s'effectue en trois étapes :\n",
    "  - **Chargement des données :** Cette opération consiste simplement à charger des données depuis une source souhaité tel qu'internet, notre system de fichier local, une base de donnée privée... Nous utiliserons ici des données disponibles sur le system de fichier local du notebook. Sous langchain cet operation peut être effectuée à l'aide de [Document Loaders](https://python.langchain.com/docs/concepts/#document-loaders)\n",
    "  - **Diviser :** Les séparateurs de texte divisent les documents volumineux plusieurs petits documents. Cela est utile à la fois pour indexer les données et pour les transmettre à notre modèle qui dispose d'un nombre de token d'appel limité. Langchain propose pour cela des [Text splitters](https://python.langchain.com/docs/concepts/#text-splitters).\n",
    "  - **Stocker :** Le stockage et l'indexation se fait ensuite généralement sur des bases de données vectorielles à partir d'indexations faites à l'aide d'un modèle d'embeding. Langchain propose pour cela les objets [VectorStore](https://python.langchain.com/docs/concepts/#vector-stores) et [Embeddings model](https://python.langchain.com/docs/concepts/#embedding-models)\n",
    "\n",
    "  ![Indexation](images/rag_indexing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f1a0d5-f929-463c-81fa-1287396e36b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=Warning,\n",
    "    message=\".*ElasticVectorSearch.*|.* using TLS with verify_certs=False is insecure.*|.*Unverified HTTPS request.*\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716f705-0700-422b-b2e4-228d01b966e3",
   "metadata": {},
   "source": [
    "### Initialisation des identifiants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f300d86-ba0a-44bc-b45b-a7933e17f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials\n",
    "\n",
    "API_VERSION = \"2024-09-01-preview\"\n",
    "AZURE_ENDPOINT = \"\"  # TODO: Ajouter l'endpoint\n",
    "API_KEY = \"\"  # TODO: Ajouter la clé API\n",
    "\n",
    "azure_open_ai_parameters = {\n",
    "    \"api_version\": API_VERSION,\n",
    "    \"azure_endpoint\": AZURE_ENDPOINT,\n",
    "    \"api_key\": API_KEY\n",
    "}\n",
    "\n",
    "elastic_search_parameters = {\n",
    "    \"username\": \"elastic\",\n",
    "    \"password\": \"\"  # TODO: Ajouter le mot de passe elastic search\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b27d7a-14e4-4c10-a23a-8f005287320e",
   "metadata": {},
   "source": [
    "### Création de notre model d'embeding\n",
    "\n",
    "Cet embedding est basé sur un model open AI hébergé sur la plateforme Azure d'IDFM appelé à l'aide d'une API.\n",
    "\n",
    "Ici, le modèle va permettre de générer différents vecteur pour les différents documents que l'on a avant de les stocker dans notre base de données vectorielle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55f70ff-f767-4f45-a879-ee573a6f9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    **azure_open_ai_parameters,\n",
    "    model=\"test-embedding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468d878-b9c6-42ec-bc94-bc8f3395992f",
   "metadata": {},
   "source": [
    "### Creation de notre Vector Store\n",
    "\n",
    "Ici un vector store sur Elastic Search. Vous pouvez regarder la base de données Elastic Search via Onyxia.\n",
    "Vous devez créer votre index personnel avant d'insérer vos documents (exemple: prénom_nom_index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bc96bd0-8c55-4401-9ce0-c60f10ca7de7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import ElasticVectorSearch\n",
    "\n",
    "index = \"\"  # TODO: Ajouter votre index\n",
    "\n",
    "vector_store = ElasticVectorSearch(\n",
    "    elasticsearch_url=f\"https://{elastic_search_parameters[\"username\"]}:{elastic_search_parameters[\"password\"]}@elastic-826951-elasticsearch:9200\",\n",
    "    index_name=index,\n",
    "    embedding=embedding_model,\n",
    "    ssl_verify = {'verify_certs': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2369b6c5-8cff-43de-8ead-a702bed98497",
   "metadata": {},
   "source": [
    "### Initialisation de notre Document loader et splitter\n",
    "\n",
    "Le document donné est un exemple, vous pouvez ajouter des documents dans le dossier data et changer le chemin ci-dessous vers votre document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4821ee29-1652-4e93-9ec0-a5d0b4ea7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "doc_path = \"./data/documentation_referentiel_arret.txt\"  # TODO: Ajouter un document dans data et modifier le chemin\n",
    "\n",
    "loader = TextLoader(doc_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949aba1-2026-402e-8e0a-79513e3ef8f7",
   "metadata": {},
   "source": [
    "### Indexation de documents\n",
    "\n",
    "Étape d'ajout des documents à notre base de donnée vectorielle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f724e4-8d39-400a-9e08-1961b78a76b6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector_store.add_documents(documents=splits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
