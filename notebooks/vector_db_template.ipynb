{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1817534a-aa76-48e6-a4bc-5c625676f5e1",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "Nous vous proposons dans ce notebook un template alimentant notre base de données vectorielle (ici Elastic Search) fonctionnant sur la plateforme Onyxia et l'environement Azure disponible chez IDFM.\n",
    "\n",
    "## Langchain\n",
    "\n",
    "Ce template est basé sur [langchain](https://python.langchain.com/docs/introduction/), librairie devenue un standard dans l'utilisation des LLM en général.\n",
    "Le tutoriel de langchain disponible en suivant ce [lien](https://python.langchain.com/docs/tutorials/rag/)\n",
    "\n",
    "## Fonctionnement\n",
    "\n",
    "Un RAG est une technique permettant d'enrichir les connaissances des LLM avec des données supplémentaires.\n",
    "\n",
    "Les LLM peuvent raisonner sur des sujets très variés, mais leurs connaissances sont limitées aux données qui ont été utilisées pour leur entraînement. Si vous souhaitez créer des applications d'IA capables de traiter des données privées ou des données introduites après la date limite d'un modèle, vous devez enrichir les connaissances de celui-ci avec les informations spécifiques dont il a besoin. Le processus consistant à apporter les informations appropriées et à les insérer dans l'invite du modèle est connu sous le nom de \"Retrieval Augmented Generation\" (RAG).\n",
    "\n",
    "LangChain dispose d'un certain nombre de composants conçus pour faciliter la création d'applications de questions-réponses et, plus généralement, d'applications de RAG.\n",
    "\n",
    "Le fonctionnement standard d'un RAG fait intervenir deux étapes :\n",
    "  - **L'indexation :** Phase au cours de laquelle nous déposons et indexons notre corpus de documents dans la base de données utilisée.\n",
    "  - **Le retrieval & generation :** Phase qui nous permet d'executer le RAG et de l'appeler avec un prompt.\n",
    "\n",
    "Ici, nous allons voir la partie **Indexation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a903f5-0509-495a-90c2-2f904d1b2599",
   "metadata": {},
   "source": [
    "## Indexation\n",
    "\n",
    "L'indexation s'effectue en trois étapes :\n",
    "  - **Chargement des données :** Cette opération consiste simplement à charger des données depuis une source souhaité tel qu'internet, notre system de fichier local, une base de donnée privée... Nous utiliserons ici des données disponibles sur le system de fichier local du notebook. Sous langchain cet operation peut être effectuée à l'aide de [Document Loaders](https://python.langchain.com/docs/concepts/#document-loaders)\n",
    "  - **Diviser :** Les séparateurs de texte divisent les documents volumineux plusieurs petits documents. Cela est utile à la fois pour indexer les données et pour les transmettre à notre modèle qui dispose d'un nombre de token d'appel limité. Langchain propose pour cela des [Text splitters](https://python.langchain.com/docs/concepts/#text-splitters).\n",
    "  - **Stocker :** Le stockage et l'indexation se fait ensuite généralement sur des bases de données vectorielles à partir d'indexations faites à l'aide d'un modèle d'embeding. Langchain propose pour cela les objets [VectorStore](https://python.langchain.com/docs/concepts/#vector-stores) et [Embeddings model](https://python.langchain.com/docs/concepts/#embedding-models)\n",
    "\n",
    "  ![Indexation](images/rag_indexing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed434850-c073-4857-8258-368119b24dca",
   "metadata": {},
   "source": [
    "## Faire un bon embedding\n",
    "\n",
    "\n",
    "### Conservation du contexte\n",
    "\n",
    "- **Petits segments (chunk_size) :** Si vous segmentez un document en morceaux trop petits, chaque segment peut perdre une partie du contexte. Par exemple, diviser un paragraphe en phrases individuelles peut rendre difficile la capture de la signification globale.\n",
    "- **Grands segments (chunk_size) :** Si les segments sont trop grands, vous risquez de dépasser les limites du modèle ou d'incorporer trop d'informations non pertinentes, ce qui peut diluer l'embedding.\n",
    "\n",
    "\n",
    "### Types de documents et objectifs\n",
    "\n",
    "Le choix du *chunk_size* dépend aussi du type de document que vous traitez et de vos objectifs.\n",
    "\n",
    "- **Textes longs (articles, livres, etc.) :** Ici, il est souvent recommandé de segmenter par paragraphe ou bloc sémantiquement cohérent (comme une section). Cela permet de conserver du contexte tout en gardant chaque chunk suffisamment petit.\n",
    "\n",
    "- **Textes courts/Question-réponse ou recherche d'information :** Si vous prévoyez d'utiliser les embeddings pour rechercher des réponses précises, un chunk de taille moyenne, correspondant à un paragraphe ou quelques phrases (par exemple, 100-300 tokens), peut fournir un bon équilibre entre contexte et spécificité.\n",
    "\n",
    "\n",
    "### Techniques pour améliorer les embeddings avec des chunks\n",
    "\n",
    "- **Sliding window :** Utiliser une fenêtre glissante permet de créer des segments qui se chevauchent légèrement. Cela garantit que les phrases situées aux limites d'un chunk ne perdent pas leur contexte. Par exemple, si vous avez un chunk de 200 tokens, vous pouvez faire un chevauchement de 50 tokens avec le chunk suivant. Pour cela vous pouvez utiliser le paramètre *chunk_overlap*.\n",
    "\n",
    "- **Segmentation naturelle :** Diviser en plusieurs documents si vous voulez insérer des informations sur divers sujets dans votre index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e72f7d-88a1-4092-903d-2dd5ace107b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=Warning,\n",
    "    message=\".*ElasticVectorSearch.*|.* using TLS with verify_certs=False is insecure.*|.*Unverified HTTPS request.*\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716f705-0700-422b-b2e4-228d01b966e3",
   "metadata": {},
   "source": [
    "### Initialisation des identifiants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c804197f-9b66-4c13-b487-934291100d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "API_VERSION = \"2024-09-01-preview\"\n",
    "AZURE_ENDPOINT = os.getenv('AZURE_ENDPOINT')\n",
    "API_KEY = os.getenv('OPEN_API_KEY')\n",
    "\n",
    "azure_open_ai_parameters = {\n",
    "    \"api_version\": API_VERSION,\n",
    "    \"azure_endpoint\": AZURE_ENDPOINT,\n",
    "    \"api_key\": API_KEY\n",
    "}\n",
    "\n",
    "elastic_search_parameters = {\n",
    "    \"username\": \"elastic\",\n",
    "    \"password\": os.getenv('ELASTICSEARCH_PASSWORD')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b27d7a-14e4-4c10-a23a-8f005287320e",
   "metadata": {},
   "source": [
    "### Création de notre model d'embeding\n",
    "\n",
    "Cet embedding est basé sur un model open AI hébergé sur la plateforme Azure d'IDFM appelé à l'aide d'une API.\n",
    "\n",
    "Ici, le modèle va permettre de générer différents vecteur pour les différents documents que l'on a avant de les stocker dans notre base de données vectorielle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea5b45-527f-4d34-841b-f40b39f79b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    **azure_open_ai_parameters,\n",
    "    model=\"test-embedding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468d878-b9c6-42ec-bc94-bc8f3395992f",
   "metadata": {},
   "source": [
    "### Creation de notre Vector Store\n",
    "\n",
    "Ici un vector store sur Elastic Search. Vous pouvez regarder la base de données Elastic Search via Onyxia.\n",
    "Vous devez créer votre index personnel avant d'insérer vos documents (exemple: prénom_nom_index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8a256-baab-485e-91e3-f36972f9de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import ElasticVectorSearch\n",
    "\n",
    "index = \"\"  # TODO: Ajouter votre index\n",
    "\n",
    "vector_store = ElasticVectorSearch(\n",
    "    elasticsearch_url=f\"https://{elastic_search_parameters[\"username\"]}:{elastic_search_parameters[\"password\"]}@elastic-826951-elasticsearch:9200\",\n",
    "    index_name=index,\n",
    "    embedding=embedding_model,\n",
    "    ssl_verify = {'verify_certs': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2369b6c5-8cff-43de-8ead-a702bed98497",
   "metadata": {},
   "source": [
    "### Initialisation de notre Document loader et splitter\n",
    "\n",
    "Le document donné est un exemple, vous pouvez ajouter des documents dans le dossier data et changer le chemin ci-dessous vers votre document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8c224-d0c3-43f5-8e39-46fa09810a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "doc_path = \"./data/documentation_referentiel_arret.txt\"  # TODO: Ajouter un document dans data et modifier le chemin\n",
    "\n",
    "loader = TextLoader(doc_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949aba1-2026-402e-8e0a-79513e3ef8f7",
   "metadata": {},
   "source": [
    "### Indexation de documents\n",
    "\n",
    "Étape d'ajout des documents à notre base de donnée vectorielle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb489a8-c998-43bc-9df4-09e1871b0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860d3d41-3d20-4828-8e81-b9329fb2dfd6",
   "metadata": {},
   "source": [
    "### Elasticsearch\n",
    "\n",
    "Après avoir créé votre index, vous pouvez aller sur la page graphique d'Elasticsearch [ici](https://dlb-deptdata-826951.data-platform-self-service.net/app/enterprise_search/content/search_indices) pour voir les documents insérés dedans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095e851-2f17-4034-afc2-f797f892fdbc",
   "metadata": {},
   "source": [
    "### ⚠️ Réinitialisation de la base de données ⚠️\n",
    "\n",
    "Si jamais vous avez ajouté plusiseurs fois les mêmes documents, ou si vous voulez tester de nouvelles choses, vous pouvez supprimer votre index à l'aide de ce code.\n",
    "\n",
    "**⚠️ Attention, ce code supprime l'index d'Elasticsearch !!! ⚠️**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d9d43-473f-4bf7-bca5-4b5dd6cc88ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es_client = Elasticsearch(\n",
    "    hosts=[f\"https://{elastic_search_parameters[\"username\"]}:{elastic_search_parameters[\"password\"]}@elastic-826951-elasticsearch:9200\"],\n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "if es_client.indices.exists(index=index):\n",
    "    es_client.indices.delete(index=index)\n",
    "    print(f\"L'index '{index}' a été supprimé.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
