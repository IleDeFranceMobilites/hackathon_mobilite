{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "Nous vous proposons dans ce notebook un template de RAG fonctionnant sur la plateforme Onyxia et l'environement Azure disponible chez IDFM.\n",
    "Vous pourrez vous inspirer de ce template pour toutes vos solutions basées sur de l'IA générative.\n",
    "\n",
    "## ⚠️ Warning ⚠️\n",
    "\n",
    "Attention, une base de données préremplie est à votre disposition. Mais si vous voulez ajouter des documents, il est nécessaire d'effectuer une étape d'indexation. Nous proposons un exemple d'implementation de cette étape dans ce [notebook](./vector_db_template.ipynb).\n",
    "\n",
    "## Langchain\n",
    "\n",
    "Ce template est basé sur [langchain](https://python.langchain.com/docs/introduction/), librairie devenue un standard dans l'utilisation des LLM en général.\n",
    "Le tutoriel de langchain disponible en suivant ce [lien](https://python.langchain.com/docs/tutorials/rag/)\n",
    "\n",
    "## Fonctionnement\n",
    "\n",
    "Un RAG est une technique permettant d'enrichir les connaissances des LLM avec des données supplémentaires.\n",
    "\n",
    "Les LLM peuvent raisonner sur des sujets très variés, mais leurs connaissances sont limitées aux données qui ont été utilisées pour leur entraînement. Si vous souhaitez créer des applications d'IA capables de traiter des données privées ou des données introduites après la date limite d'un modèle, vous devez enrichir les connaissances de celui-ci avec les informations spécifiques dont il a besoin. Le processus consistant à apporter les informations appropriées et à les insérer dans l'invite du modèle est connu sous le nom de \"Retrieval Augmented Generation\" (RAG).\n",
    "\n",
    "LangChain dispose d'un certain nombre de composants conçus pour faciliter la création d'applications de questions-réponses et, plus généralement, d'applications de RAG.\n",
    "\n",
    "Le fonctionnement standard d'un RAG fait intervenir deux étapes :\n",
    "  - **L'indexation :** Phase au cours de laquelle nous déposons et indexons notre corpus de documents dans la base de données utilisée.\n",
    "  - **Le retrieval & generation :** Phase qui nous permet d'executer le RAG et de l'appeler avec un prompt.\n",
    "\n",
    "Ici, nous allons voir la partie **Retrieval et Generation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval & Generation\n",
    "\n",
    "L'étape qui nous permet d'executer notre RAG à proprement parler.\n",
    "\n",
    "- **Retrieval :** À partir d'un prompt formé par l'utilisateur, le vector store renvoie les documents (ou extraits de documents) pertinent dans le contexte de la question. Nous utilisons ici un [Retriever](https://python.langchain.com/docs/concepts/#retrievers)\n",
    "- **Generation :** L'appel de notre LLM en lui fournissant le contexte retourné par le retriever et la question de l'utilisateur. Nous utilisons pour cela un [ChatModel](https://python.langchain.com/docs/concepts/#chat-models).\n",
    "\n",
    "\n",
    "![Indexation](images/rag_retrieval_generation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=Warning,\n",
    "    message=\".*ElasticVectorSearch.*|.* using TLS with verify_certs=False is insecure.*|.*Unverified HTTPS request.*\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation des identifiants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials\n",
    "\n",
    "API_VERSION = \"2024-09-01-preview\"\n",
    "AZURE_ENDPOINT = \"\"  # TODO: Ajouter l'endpoint\n",
    "API_KEY = \"\"  # TODO: Ajouter la clé API\n",
    "\n",
    "azure_open_ai_parameters = {\n",
    "    \"api_version\": API_VERSION,\n",
    "    \"azure_endpoint\": AZURE_ENDPOINT,\n",
    "    \"api_key\": API_KEY\n",
    "}\n",
    "\n",
    "elastic_search_parameters = {\n",
    "    \"username\": \"elastic\",\n",
    "    \"password\": \"\"  # TODO: Ajouter le mot de passe elastic search\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de notre model d'embedding\n",
    "\n",
    "Cet embedding est basé sur un model open AI hébergé sur la plateforme Azure d'IDFM appelé à l'aide d'une API.\n",
    "\n",
    "Ici, ce modèle va permettre de transformer notre question en vecteur pour le comparer à notre base de données vectorielle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    **azure_open_ai_parameters,\n",
    "    model=\"test-embedding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation de notre Vector Store\n",
    "\n",
    "Ici un vector store sur Elastic Search.\n",
    "Il est pour l'instant sur l'index global, après avoir ajouté des documents via [ce notebook](./vector_db_template.ipynb), vous pouvez changer l'index et mettre le votre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import ElasticVectorSearch\n",
    "\n",
    "index = \"referentiel_arret\"  # TODO: Changer cet index par le votre\n",
    "\n",
    "vector_store = ElasticVectorSearch(\n",
    "    elasticsearch_url=f\"https://{elastic_search_parameters[\"username\"]}:{elastic_search_parameters[\"password\"]}@elastic-826951-elasticsearch:9200\",\n",
    "    index_name=index,\n",
    "    embedding=embedding_model,\n",
    "    ssl_verify = {'verify_certs': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciation de notre Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciation de notre Chat Model\n",
    "Nous utilisons ici un model GPT-4 hébergé sur la plateforme Azure d'IDFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    **azure_open_ai_parameters,\n",
    "    model=\"test-onyxia-llm-model\",\n",
    "    temperature=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de notre RAG\n",
    "\n",
    "### Prompt\n",
    "Nous proposons ici un prompt personnalisé, on constate la présence des champs **{context}** et **{question}**, il s'agit des endroits ou l'on renseignera le contexte issu de notre corpus obtenu à l'aide du retriever, et la question de l'utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Tu es un agent d'assistance de recherche documentaire d’île de France mobilité (IDFM),\n",
    "à partir des documents donnés donne toujours une réponse provenant du contexte.\n",
    "Ne donne pas de réponse si le contexte ne te le permet pas.\n",
    "Répond de manière concise, en trois phrases maximum.\n",
    "Répond dans la langue de la question\n",
    "Inclut toujours un extrait textuel des documents.\n",
    "\n",
    "On dispose du contexte suivants : {context}\n",
    "\n",
    "Utilisateur : \"{question}\"\n",
    "Chat bot :\n",
    "\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemblage du RAG\n",
    "\n",
    "Nous enchaînons ici les étapes successives nécessaire à l'execution du RAG, les objets RunnablePassthrough et StrOutputParser. L'objet [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) permettent de configurer plus finement l'appel de notre RAG, ici il se comporte comme une fonction identité. Le [StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) permet simplement d'extraire la réponse du LLM afin de pouvoir l'afficher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | print\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appel de notre rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"Quel est le niveau d'agregation au dessus des zones d'arret ?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
